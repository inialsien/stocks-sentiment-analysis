{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from senti_classifier import senti_classifier\n",
    "from TwitterAPI import TwitterAPI\n",
    "from datetime import datetime\n",
    "import ConfigParser\n",
    "import pickle\n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established Twitter connection.\n"
     ]
    }
   ],
   "source": [
    "# Connect to the Twitter API and return a TwitterAPI object to use.\n",
    "def get_twitter(config_file):\n",
    "    config = ConfigParser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    twitter = TwitterAPI(\n",
    "                   config.get('twitter', 'consumer_key'),\n",
    "                   config.get('twitter', 'consumer_secret'),\n",
    "                   config.get('twitter', 'access_token'),\n",
    "                   config.get('twitter', 'access_token_secret'))\n",
    "    return twitter\n",
    "\n",
    "twitter = get_twitter('twitter.cfg')\n",
    "print('Established Twitter connection.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "def extract_id(tweet_request_response, min_id=998816577052196870):\n",
    "    \"\"\"\n",
    "    Extract the min of tweets' id, use a bad hack to initialize max_id\n",
    "    \"\"\"\n",
    "    for tweet in tweet_request_response:\n",
    "        if tweet['id'] < min_id:\n",
    "            min_id = tweet['id']\n",
    "        \n",
    "    return min_id\n",
    "\n",
    "def gather_tweets(search_query, since, until):\n",
    "    tweets = twitter.request('search/tweets',{'q':search_query+' since:'+since+' until:'+until, 'lang':'en', 'count':100})\n",
    "    min_id = extract_id(tweets)\n",
    "    tweets_list=[]\n",
    "    \n",
    "    while True:\n",
    "            if not tweets.get_rest_quota():\n",
    "                return tweets_list\n",
    "        \n",
    "            try:\n",
    "                tweets = twitter.request('search/tweets',\n",
    "                             {'q' : search_query+' since:'+since+' until:'+until, \n",
    "                              'lang' : 'en', \n",
    "                              'max_id' : min_id-1,\n",
    "                              'count' : 100})\n",
    "            except:\n",
    "                print 'No more quota !'\n",
    "                return tweets_list\n",
    "            \n",
    "            for tweet in tweets:\n",
    "                process_text = process_tweet(tweet['text'])\n",
    "                tweets_list.append({'text': process_text, \n",
    "                                    'author': tweet['user']['name'], \n",
    "                                    'date': datetime.strptime(tweet['created_at'], '%a %b %d %H:%M:%S +0000 %Y').strftime('%Y-%m-%d'),\n",
    "                                    'time': datetime.strptime(tweet['created_at'], '%a %b %d %H:%M:%S +0000 %Y').strftime('%H:%M:%S'),\n",
    "                                   })\n",
    "                \n",
    "            if (extract_id(tweets, min_id=min_id) == min_id):\n",
    "                return tweets_list\n",
    "            else:\n",
    "                min_id = extract_id(tweets, min_id=min_id)\n",
    "            \n",
    "            print 'Next max_id: ' + str(min_id)\n",
    "            print 'Remaining quota: ' + str(tweets.get_rest_quota()['remaining'])\n",
    "    \n",
    "    return tweets_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next max_id: 673896739233927169\n",
      "Remaining quota: 178\n",
      "Next max_id: 673830003436421120\n",
      "Remaining quota: 177\n",
      "Next max_id: 673680056447844354\n",
      "Remaining quota: 176\n",
      "Next max_id: 673654578261434368\n",
      "Remaining quota: 175\n",
      "Period 2015-12-07 to 2015-12-08 gathered.\n",
      "Period 2015-12-07 to 2015-12-08 written in file.\n",
      "300\n",
      "Next max_id: 674269723928973312\n",
      "Remaining quota: 172\n",
      "Next max_id: 674206600446496769\n",
      "Remaining quota: 171\n",
      "Next max_id: 674052757842608128\n",
      "Remaining quota: 170\n",
      "Next max_id: 674015902438842369\n",
      "Remaining quota: 169\n",
      "Period 2015-12-08 to 2015-12-09 gathered.\n",
      "Period 2015-12-08 to 2015-12-09 written in file.\n",
      "324\n"
     ]
    }
   ],
   "source": [
    "def dump_data_stock(period, stock, path):\n",
    "    tweets_dico_list = gather_tweets(stock, period[0], period[1])\n",
    "    print 'Period ' + period[0] + ' to ' + period[1] + ' gathered.'\n",
    "    pickle.dump(tweets_dico_list, open(path, 'wb' ))\n",
    "    print 'Period ' + period[0] + ' to ' + period[1] + ' written in file.'\n",
    "    print len(tweets_dico_list)\n",
    "    return dump_data_stock\n",
    "    \n",
    "periods = [['2015-11-30', '2015-12-01'], \n",
    "           ['2015-12-01', '2015-12-02'], \n",
    "           ['2015-12-02', '2015-12-03'], \n",
    "           ['2015-12-03', '2015-12-04'], \n",
    "           ['2015-12-04', '2015-12-05'], \n",
    "           ['2015-12-05', '2015-12-06'],\n",
    "           ['2015-12-06', '2015-12-07']]\n",
    "\n",
    "periods = [['2015-12-07', '2015-12-08'],\n",
    "           ['2015-12-08', '2015-12-09']]\n",
    "\n",
    "for period in periods:\n",
    "    path = 'data_google'+ os.sep + 'save-' + period[0] + '-to-' + period[1] + '.pkl'\n",
    "    dump_data_stock(period, '%24GOOG', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_feeling_avg(tweets_dico_list):\n",
    "    texts_list = []\n",
    "    for tweet in tweets_dico_list:\n",
    "        texts_list.append(tweet['text'])\n",
    "    print 'list finished'\n",
    "    return senti_classifier.polarity_scores(texts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_csv(periods, pre_path, csv_file):\n",
    "    results_list = []\n",
    "    \n",
    "    for period in periods:\n",
    "        tweets_dico_list = pickle.load(open( pre_path + 'save-' + period[0] + '-to-' + period[1] + '.pkl', \"rb\" ))\n",
    "        feeling = compute_feeling_avg(tweets_dico_list)\n",
    "        results_list.append([tweets_dico_list[0]['date'], len(tweets_dico_list), feeling[0], feeling[1]])\n",
    "    \n",
    "    print results_list\n",
    "    \n",
    "    with open(csv_file, \"wb\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(results_list)\n",
    "        \n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list finished\n",
      "list finished\n",
      "list finished\n",
      "list finished\n",
      "list finished\n",
      "list finished\n",
      "list finished\n",
      "list finished\n",
      "list finished\n",
      "[['2015-11-30', 343, 61.75, 40.5], ['2015-12-01', 487, 92.75, 45.125], ['2015-12-02', 679, 115.375, 53.125], ['2015-12-03', 508, 101.375, 37.125], ['2015-12-04', 256, 76.25, 26.125], ['2015-12-05', 114, 21.625, 12.875], ['2015-12-06', 147, 34.125, 7.625], ['2015-12-07', 300, 54.0, 19.375], ['2015-12-08', 324, 70.75, 28.625]]\n"
     ]
    }
   ],
   "source": [
    "periods = [['2015-11-30', '2015-12-01'],\n",
    "           ['2015-12-01', '2015-12-02'],\n",
    "           ['2015-12-02', '2015-12-03'],\n",
    "           ['2015-12-03', '2015-12-04'],\n",
    "           ['2015-12-04', '2015-12-05'],\n",
    "           ['2015-12-05', '2015-12-06'],\n",
    "           ['2015-12-06', '2015-12-07'],\n",
    "           ['2015-12-07', '2015-12-08'],\n",
    "           ['2015-12-08', '2015-12-09']]\n",
    "pre_path = './data_google/'\n",
    "csv_file = './csv/csv_google_1.csv'\n",
    "\n",
    "result_google = create_csv(periods, pre_path, csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results GOOGLE\n",
    "- Period 2015-11-23 to 2015-11-24 = (692.625, 432.375)\n",
    "- Period 2015-11-24 to 2015-11-25 = (709.5, 446.0)\n",
    "- Period 2015-11-25 to 2015-11-26 = (726.375, 459.625)\n",
    "- Period 2015-11-26 to 2015-11-27 = (163.125, 101.625)\n",
    "- Period 2015-11-29 to 2015-11-30 = (675.75, 418.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
